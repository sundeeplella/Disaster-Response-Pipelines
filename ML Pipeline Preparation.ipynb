{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# basic data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# scikit-learn modules for pipelining, transformation, model fitting and classification\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# nltk-modules for text processing, tokenizing and lemmatizing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download relevant ntlk packages\n",
    "nltk.download([\"punkt\", \"stopwords\", \"wordnet\"])\n",
    "\n",
    "# pickle for python object serialization and storing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipython --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-e6878a2acfe0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-e6878a2acfe0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    install andas --upgrade\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database.\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table('messages', engine)\n",
    "X = df.loc[:,\"message\"]\n",
    "Y = df.iloc[:,4:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    Tokenize, lemmatize, lower and remove punctuation of input text.\n",
    "    Args:\n",
    "    text: single string with input text\n",
    "    \n",
    "    Returns:\n",
    "    output: List of processed string after tokenization\n",
    "    '''\n",
    "    #set text to lower case and remove punctuation\n",
    "    text = re.sub(\"[\\W_]\",\" \",text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    #tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    #lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #init and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    output = [lemmatizer.lemmatize(w) for w in tokens if not w in stop_words]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(\"For today:= this is, a advanced _ example #- String!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(n_estimators=100)))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"\"\"\n",
    "Shape of all elements: \\n\\t X_train: {}\n",
    "\\t X_test: {}\n",
    "\\t Y_train: {}\n",
    "\\t Y_test: {}\"\"\".format(X_train.shape,\n",
    "                       X_test.shape,\n",
    "                       Y_train.shape,\n",
    "                       Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train pipeline\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_arr_df(two_d_data):\n",
    "    \"\"\"\n",
    "    Flatten array/list of arrays/lists and dataframes to lists.\n",
    "\n",
    "    Input arguments:\n",
    "        two_d_data: dataframe or array/list of arrays/lists \n",
    "                    Example: [[1,2,3],[4,5,6],[7,8,9]]\n",
    "              \n",
    "    Output:\n",
    "        flat_data: List of flattened Input\n",
    "                   Example: [1,2,3,4,5,6,7,8,9]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(two_d_data, (list, np.ndarray)):\n",
    "        if isinstance(two_d_data[0], (list, np.ndarray)):\n",
    "            flat_data = [item for sublist in two_d_data for item in sublist]\n",
    "        else:\n",
    "            print(\"Wrong datatype used, cannot flat this object\")\n",
    "            return \"\"\n",
    "    elif isinstance(two_d_data, pd.DataFrame):\n",
    "            flat_data = list(two_d_data.values.flatten())\n",
    "    \n",
    "    return flat_data\n",
    "\n",
    "def return_multioutput_f1_prec_recall(Y_pred, Y_test, mac_avg=False):\n",
    "    \"\"\"\n",
    "    Output classification report (f1, precision, recall) seperated for every category in\n",
    "    Multi_Output_Data\n",
    "\n",
    "    Input arguments:\n",
    "        Y_pred: dataframe or array/list of arrays/lists \n",
    "                    Example: [[1,2,3],[4,5,6],[7,8,9]]\n",
    "        \n",
    "        Y_test: dataframe or array/list of arrays/lists \n",
    "                    Example: [[1,2,3],[4,5,6],[7,8,9]]\n",
    "                    \n",
    "        mac_avg: If True returns Dict with Category names and F1 Score\n",
    "              \n",
    "    Output:\n",
    "        If mac_avg==False: prints precision, recall and f1-score\n",
    "        If mac_avg==True: returns Dict with Category names and F1 Score\n",
    "    \n",
    "    \"\"\"\n",
    "    if mac_avg:\n",
    "        mac_avg_dict = {}\n",
    "        \n",
    "    for pred_row, test_row, colname in zip(Y_pred.T, Y_test.to_numpy().T, Y_test.columns):\n",
    "        if mac_avg:\n",
    "            mac_avg_dict[colname] = classification_report(pred_row, test_row, output_dict=True)[\"macro avg\"][\"f1-score\"]\n",
    "        else:\n",
    "            print(\"Report for Category: {}\".format(colname))\n",
    "            print(classification_report(pred_row, test_row))    \n",
    "    \n",
    "    if mac_avg:\n",
    "        return mac_avg_dict\n",
    "    \n",
    "def return_flatted_f1_prec_recall(Y_pred, Y_test, mac_avg=False):\n",
    "    \"\"\"\n",
    "    Output classification report (f1, precision, recall) for flatted prediction and test data.\n",
    "\n",
    "    Input arguments:\n",
    "        Y_pred: dataframe or array/list of arrays/lists \n",
    "                    Example: [[1,2,3],[4,5,6],[7,8,9]]\n",
    "        \n",
    "        Y_test: dataframe or array/list of arrays/lists \n",
    "                    Example: [[1,2,3],[4,5,6],[7,8,9]]\n",
    "                    \n",
    "        mac_avg: If True returns F1-Score\n",
    "              \n",
    "    Output:\n",
    "        If mac_avg==False: prints precision recall and f1-score\n",
    "        If mac_avg==True: returns F1-Score only\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    flat_Y_pred = flat_arr_df(Y_pred)\n",
    "    flat_Y_test = flat_arr_df(Y_test)\n",
    "    if mac_avg:\n",
    "        return classification_report(flat_Y_pred, flat_Y_test, output_dict=True)[\"macro avg\"][\"f1-score\"]\n",
    "    else:\n",
    "        print(classification_report(flat_Y_pred, flat_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_arr = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "test_data_df = pd.DataFrame(test_data_arr)\n",
    "flat_arr_df(test_data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_arr_df(test_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00         1\n",
      "          2       1.00      1.00      1.00         1\n",
      "          3       1.00      1.00      1.00         1\n",
      "          4       1.00      1.00      1.00         1\n",
      "          5       1.00      1.00      1.00         1\n",
      "          6       1.00      1.00      1.00         1\n",
      "          7       1.00      1.00      1.00         1\n",
      "          8       1.00      1.00      1.00         1\n",
      "          9       1.00      1.00      1.00         1\n",
      "\n",
      "avg / total       1.00      1.00      1.00         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "return_flatted_f1_prec_recall(test_data_arr,test_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_01 = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.96      0.97    264999\n",
      "          1       0.53      0.82      0.64     16125\n",
      "\n",
      "avg / total       0.96      0.95      0.95    281124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "return_flatted_f1_prec_recall(Y_pred_01,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-70967f12de56>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-70967f12de56>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip3 install --upgrade pandas\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip3 install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cla_report_rfc_b = pd.Series(return_multioutput_f1_prec_recall(Y_pred_01, Y_test, mac_avg=True))\n",
    "cla_report_rfc_b = cla_report_rfc_b.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cla_report_rfc_b.rename(columns={0:\"F1 Score for RandomForestClassifier without Modification\"}, inplace=True)\n",
    "_ = cla_report_rfc_b.plot(kind=\"bar\", figsize=(15,5))\n",
    "_ = plt.title(\"Macro Averaged F1-Score for several Model Attempts\")\n",
    "_ = plt.ylabel(\"Macro Averaged F1-Score\")\n",
    "_ = plt.xlabel(\"Multi-Output Categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(n_estimators=100)))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'vect', 'tfidf', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'clf__estimator__bootstrap', 'clf__estimator__class_weight', 'clf__estimator__criterion', 'clf__estimator__max_depth', 'clf__estimator__max_features', 'clf__estimator__max_leaf_nodes', 'clf__estimator__min_impurity_decrease', 'clf__estimator__min_impurity_split', 'clf__estimator__min_samples_leaf', 'clf__estimator__min_samples_split', 'clf__estimator__min_weight_fraction_leaf', 'clf__estimator__n_estimators', 'clf__estimator__n_jobs', 'clf__estimator__oob_score', 'clf__estimator__random_state', 'clf__estimator__verbose', 'clf__estimator__warm_start', 'clf__estimator', 'clf__n_jobs'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "        'clf__estimator__min_samples_split': [2, 3, 4],\n",
    "        'clf__estimator__n_estimators': [100, 200, 300],\n",
    "        'clf__estimator__criterion': ['entropy', 'gini']\n",
    "    }\n",
    "\n",
    "cv =  GridSearchCV(pipeline, param_grid=parameters, verbose=3, n_jobs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=100 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=200 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=200 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=200 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=300 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=300 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=300 \n",
      "[CV] clf__estimator__criterion=entropy, clf__estimator__min_samples_split=3, clf__estimator__n_estimators=100 \n"
     ]
    }
   ],
   "source": [
    "cv.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_02 = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_flatted_f1_prec_recall(Y_pred_02,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cla_report_rfc_a = pd.Series(return_multioutput_f1_prec_recall(Y_pred_02, Y_test,mac_avg=True))\n",
    "cla_report_rfc_a = cla_report_rfc_a.to_frame()\n",
    "\n",
    "cla_report_rfc_a.rename(columns={0:\"F1 Score for RandomForestClassifier with Optimized Parameters\"}, inplace=True)\n",
    "cla_report_rfc_a = pd.concat([cla_report_rfc_b, cla_report_rfc_a], axis=1)\n",
    "_ = cla_report_rfc_a.plot(kind=\"bar\", figsize=(15,5))\n",
    "_ = plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "_ = plt.title(\"Macro Averaged F1-Score for several Model Attempts\")\n",
    "_ = plt.ylabel(\"Macro Averaged F1-Score\")\n",
    "_ = plt.xlabel(\"Multi-Output Categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageIsQuestion(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Extract messages that starts with question word or ends with question mark and\n",
    "    return DataFrame with 1's for True and 0's for False\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Create the Regex Variable for checking\n",
    "        '''\n",
    "        # typical englisch question words\n",
    "        question_words = [\"what\", \"when\", \"do\", \"is\", \"who\", \"which\", \"where\", \"why\", \"how\"]\n",
    "        # matches question words at beginning of text or questionmarks at the end\n",
    "        question_reg = \"(^\"+\"(\"+\"|\".join(question_words)+\")|(\\?)$)\"\n",
    "        self.q_reg = re.compile(question_reg)\n",
    "    \n",
    "    def message_question(self, text):\n",
    "        '''\n",
    "        Will get on text message per execution. After Tokenizing by sentences, it will return 1\n",
    "        on matching the regex question_reg or 0 for not matching.\n",
    "        \n",
    "        Input Arguments:\n",
    "            text: Single String with message\n",
    "            \n",
    "        Output:\n",
    "            output: Returns 1 if text includes Question and returns 0 when Question doesn't include question\n",
    "        \n",
    "        '''\n",
    "        # tokenize by sentences\n",
    "        sentence_list =  nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            # find pattern question_reg in each sentence\n",
    "            sentence = sentence.lower()\n",
    "            if self.q_reg.match(sentence):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        Will go through single column DataFrame and applies message_question to every message in the row.\n",
    "        The returning DataFrame holds 1's or 0's whether the message includes question or not.\n",
    "        \n",
    "        Input Arguments:\n",
    "            X: DataFrame with single column or array/list\n",
    "        \n",
    "        Output:\n",
    "            output: Cleaned DataFrame with 1's and 0's for messages\n",
    "        '''\n",
    "        \n",
    "        # apply message_question function to all values in X\n",
    "        X_tagged = pd.Series(X).apply(self.message_question)\n",
    "        \n",
    "        # clean the resulting Dataframe that it can be processed through the pipeline\n",
    "        df = pd.DataFrame(X_tagged)\n",
    "        df.fillna(0, inplace=True)\n",
    "        df = df.astype(int)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miq = MessageIsQuestion()\n",
    "output = miq.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_ct_pipe = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('message_question', MessageIsQuestion())\n",
    "        ])),\n",
    "\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(n_estimators=300,\n",
    "                                                             min_samples_split=3,\n",
    "                                                             criterion=\"gini\"))\n",
    "    )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = rfc_ct_pipe.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_03 = rfc_ct_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_flatted_f1_prec_recall(Y_pred_03, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cla_report_rfc_ct=pd.Series(return_multioutput_f1_prec_recall(Y_pred_03, Y_test,mac_avg=True))\n",
    "cla_report_rfc_ct = cla_report_rfc_ct.to_frame()\n",
    "cla_report_rfc_ct.rename(columns={0:\"F1-Score for RandomForestClassifier with Custom Transfomer\"}, inplace=True)\n",
    "cla_report_rfc_ct = pd.concat([cla_report_rfc_b, cla_report_rfc_ct], axis=1)\n",
    "_ = cla_report_rfc_ct.plot(kind=\"bar\", figsize=(15,5))\n",
    "_ = plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "_ = plt.title(\"Macro Averaged F1-Score for several Model Attempts\")\n",
    "_ = plt.ylabel(\"Macro Averaged F1-Score\")\n",
    "_ = plt.xlabel(\"Multi-Output Categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html?highlight=nlp\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(MultinomialNB()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_04 = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_flatted_f1_prec_recall(Y_pred_04, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cla_report_mnb=pd.Series(return_multioutput_f1_prec_recall(Y_pred_04, Y_test,mac_avg=True))\n",
    "cla_report_mnb = cla_report_mnb.to_frame()\n",
    "cla_report_mnb.rename(columns={0:\"F1-Score for Multinominal Naive Bayes\"}, inplace=True)\n",
    "cla_report_mnb = pd.concat([cla_report_mnb, cla_report_rfc_ct], axis=1)\n",
    "_ = cla_report_mnb.plot(kind=\"bar\", figsize=(15,5))\n",
    "_ = plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "_ = plt.title(\"Macro Averaged F1-Score for several Model Attempts\")\n",
    "_ = plt.ylabel(\"Macro Averaged F1-Score\")\n",
    "_ = plt.xlabel(\"Multi-Output Categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rfc_ct_pipe, open(\"rfc_ct_classifier.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if len(sys.argv) >= 3:\n",
    "        \n",
    "        # get input from args: \n",
    "        database_filepath, model_filepath, is_best_model = sys.argv[1:]\n",
    "        print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "\n",
    "        # load database:\n",
    "        X, Y, category_names = load_data(database_filepath)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "        \n",
    "        # build model: \n",
    "        print('Building model...')\n",
    "        model = build_model(best_estimator=is_best_model)\n",
    "        \n",
    "        # train model: \n",
    "        print('Training model...')\n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        # evaluate model:\n",
    "        print('Evaluating model...')\n",
    "        evaluate_model(model, X_test, Y_test, category_names)\n",
    "\n",
    "        # save model:\n",
    "        print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "        save_model(model, model_filepath)\n",
    "\n",
    "        print('Trained model saved!')\n",
    "\n",
    "    else:\n",
    "        print('Please provide the filepath of the disaster messages database '\\\n",
    "              'as the first argument, the filepath of the pickle file to '\\\n",
    "              'save the model to as the second argument, and specify'\\\n",
    "              'GridSearch as third argument.\\n\\nExample:'\\\n",
    "              'python train_classifier.py ../data/DisasterResponse.db classifier.pkl TRUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
